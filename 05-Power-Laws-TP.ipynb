{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACN 903 S5: Power Laws\n",
    "\n",
    "## Céline Comte & Fabien Mathieu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to deepen your theoretical knowledge of power laws, you can read (this is **not** mandatory):\n",
    "- MEJ Newman, **Power laws, Pareto distributions and Zipf's law**. Available on [arXiv](https://arxiv.org/abs/cond-mat/0412004).\n",
    "- A. Barabási and R. Albert, **Emergence of Scaling in Random Networks**. Available on [arXiv](https://arxiv.org/abs/cond-mat/9910332).\n",
    "- Chapter 7 from the book [Epidemics and Rumours in Complex Networks][massoulie].\n",
    "\n",
    "[massoulie]: http://www.lincs.fr/wp-content/uploads/2013/01/CUP_book_final.pdf \"Epidemics and Rumours in Complex Networks by Moez Draief and Laurent Massoulié\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Albert-Barabási generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first focus on the second version of the Albert-Barabási model introduced during the class.\n",
    "Starting with an arbitrary undirected graph that contains at least one edge, we progressively expand the graph  by adding nodes step by step. Each new node is attached to a single existing node chosen as follows:\n",
    "- with probability $\\alpha$, we perform a **uniform attachment**, meaning that all nodes can be chosen with the same probability;\n",
    "- with probability $1 - \\alpha$, we perform a **preferential attachment**, so that a node is chosen with a probability that is proportional to its degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Write a function ``alba(n = 1000, α = 0)`` that returns the vector of the node degrees in an instance of an Albert-Barabási graph of $n$ nodes. The initial graph contains two nodes that are connected together.\n",
    "\n",
    "**Hint 1:** Drawing nodes proportionally to their degree may be a computational bottleneck. Observing that the degrees in an Albert-Barabási graph of $n$ nodes sum to $2n-2$, can you build an array of size $2n-2$ such that choosing a node proportionally to its degree is equivalent to choosing an element of this array uniformly at random?\n",
    "\n",
    "**Hint 2:** The function ``bincount`` from ``numpy`` package could be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "For a value of $n$ that is as large as possible but still gives a reasonable running time (less than a dozen of seconds), plot the probability mass function and the complementary cumulative distribution function of the degree distribution for a few values of $\\alpha$ between $0$ and $1$. Compare the results to what you saw in the course.\n",
    "\n",
    "**Hint:** You may need the function ``cumsum`` from ``numpy`` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (Optional)\n",
    "\n",
    "What can you say specifically for the case $\\alpha = 1$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Using datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These are the same functions as in the practical *Distances and Clustering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## File format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The datasets are provided in the form of *zip* archives. Each *zip* archive contains the following files:\n",
    "- **_dataset_.ids** contains the actual names of the nodes (one per line, $ n $ lines in total). By convention, each node is associated to its line number (from $ 0 $ to $ n-1 $). Actual names may contain special characters (e.g. *ç*, *é*), so they are encoded with *utf-8*.\n",
    "- **_dataset_.adja** contains the adjacency list of the graph: line $ i $ (from $ 0 $ to $ n-1 $) contains, in plain ASCII, the numbers of the nodes that are neighbors of $ i $.\n",
    "- For oriented graphs, **_dataset_-t.adja** contains the adjacency list of the transposed graph: line $ i $ (from $ 0 $ to $ n-1 $) contains, in plain ASCII, the indices of the nodes that are linked by $ i $.\n",
    "\n",
    "\n",
    "Datasets will be given on a USB key. This practical assumes they are stored in **../Datasets/** relatively to your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "directory = \"../Datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Caching the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It can be a burden to parse again and again the same datas from ASCII files and convert them into a proper format. The following function will be used to stash your variable in a compressed *gz* file for further use. Thanks to Pierre-Antoine (ACN 2017-2018) for pointing out the possibility (even if his solution based on *npz* seemed to only work on his laptop for some reason, so some adjustments had to be made)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def use_cache(builder, prefix, variable = \"size\", rebuild = False, compress = False):            \n",
    "        try:\n",
    "            if rebuild:\n",
    "                raise ValueError('Value needs to be rebuilt')\n",
    "            if compress:\n",
    "                with gzip.GzipFile(directory + prefix + \"-\" + variable + \".npy.gz\", \"r\") as f:\n",
    "                    return np.load(f)\n",
    "            else:\n",
    "                return np.load(directory + prefix + \"-\" + variable + \".npy\")\n",
    "        except:\n",
    "            data = builder(prefix)\n",
    "            if compress:\n",
    "                with gzip.GzipFile(directory + prefix + \"-\" + variable + \".npy.gz\", \"w\") as f:\n",
    "                    np.save(f, data)\n",
    "            else:\n",
    "                np.save(directory + prefix + \"-\" + variable, data)\n",
    "            return data\n",
    "\n",
    "# set default behavior\n",
    "compress = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Most of the core functions below have the following behavior: first they try to load the results from an npy file if one exists, otherwise they parse the dataset to extract the information and save it in an npy file for the next use. This approach avoids re-doing the same work over and over again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following function gives the number $n$ of nodes and the total number $m$ of *oriented edges* of the graph. In the case where the graph is undirected, all edges will be counted twice ($(i,j)$ and $(j,i)$ are the same edge in an undirected graph) so the actual number of edges is $\\frac m 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_size(prefix):\n",
    "    n = 0\n",
    "    m = 0\n",
    "    with zipfile.ZipFile(directory + prefix + \".zip\") as myzip:\n",
    "        with myzip.open(prefix + \".adja\") as f:\n",
    "            for line in f:\n",
    "                n += 1\n",
    "                m += len([int(s) for s in line.split()])\n",
    "    size = array([n, m])\n",
    "    return size\n",
    "\n",
    "def get_size(prefix, rebuild = False):\n",
    "    return use_cache(build_size, prefix, variable = \"size\", rebuild = rebuild, compress = compress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us run this function to create the corresponding npy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n, m = get_size(\"dblp\")\n",
    "print(\"Number of nodes in dblp: %d\" % n)\n",
    "print(\"Number of edges in dblp: %d\" % (m // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Adjacency List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A natural way to store the adjacency list would be to use an array (or list) of arrays (or lists), such that A[i][j] would refer to the $j$th neighbor of node $i$. In practice, this structure can have some memory usage overhead, so we will store the \n",
    "adjacency list in a flat array with the function below.\n",
    "\n",
    "**Remark:** the proposed format is known in literature as Compressed Sparse Row (CSR). More details are available at https://en.wikipedia.org/wiki/Sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_adjacency(prefix):\n",
    "    n, m = get_size(prefix)\n",
    "    A = zeros(n + m + 1, dtype = int)\n",
    "    A[0] = n + 1 # Don't forget the +1!!!\n",
    "    with zipfile.ZipFile(directory + prefix + \".zip\") as myzip:\n",
    "        with myzip.open(prefix + \".adja\") as f:\n",
    "            i = 0\n",
    "            for line in f:\n",
    "                neighbors = array(line.split(), dtype = int)\n",
    "                A[i+1] = A[i] + len(neighbors)\n",
    "                A[A[i]:A[i+1]] = neighbors\n",
    "                i += 1\n",
    "    return A\n",
    "\n",
    "def get_adjacency(prefix, rebuild = False):\n",
    "    return use_cache(build_adjacency, prefix, variable = \"adjacency\", rebuild = rebuild, compress = compress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can load $A$ in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "A = get_adjacency(\"dblp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The result, ``A``, is a numpy array of integers of size $n+m+1$, organized as follows:\n",
    "- The $n+1$ first values are indexes\n",
    "- The $m$ last values are destinations\n",
    "- The neighbors of a node $i$ are stored in ``A[A[i]:A[i+1]]``\n",
    "\n",
    "The following function just returns the neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def neighbors(A, index):\n",
    "    return A[A[index]:A[index+1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In practice, just use ``A[A[i]:A[i+1]]`` if you can, it avoids calling a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Index / Name conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All the functions above assume a node is represented by an integer $0\\leq i<n$, but researchers, Wikipedia pages, and even actors have names! Let us write some functions to translate integers to names and *vice versa*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_ids(prefix):\n",
    "    n, m = get_size(prefix)\n",
    "    delimiter = zeros(n+1, dtype = int)\n",
    "    text = \"\"\n",
    "    with zipfile.ZipFile(directory + prefix + \".zip\") as myzip:\n",
    "        with myzip.open(prefix + \".ids\") as f:\n",
    "            i = 0\n",
    "            for line in codecs.iterdecode(f, 'utf8'):\n",
    "                delimiter[i+1] = delimiter[i] + len(line) - 1\n",
    "                text += line[0:-1]\n",
    "                i += 1\n",
    "    return [delimiter, text]\n",
    "    \n",
    "def get_ids(prefix = \"dblp\", rebuild = False):\n",
    "    return use_cache(build_ids, prefix, variable = \"ids\", rebuild = rebuild)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The function above returns an array ``delimiter`` of size $n+1$ and a string ``text`` that concatenates all researcher names. It uses the same principle used for the adjacency list: the name of a researcher associated to number $i$ is ``text[delimiter[i]:delimiter[i+1]]``. This allows us to do the conversion from name to index, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def index2name(index, prefix, delimiter = None, text = None):\n",
    "    if delimiter is None:\n",
    "        delimiter, text = get_ids(prefix)\n",
    "    return text[delimiter[index]:delimiter[index+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def name2index(name, prefix, delimiter = None, text = None):\n",
    "    try:\n",
    "        if delimiter is None:\n",
    "            delimiter, text = get_ids(prefix)\n",
    "        offset = text.index(name)\n",
    "        return where(delimiter == offset)[0][0]\n",
    "    except:\n",
    "        print(\"Name not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us try with some names. Note that the first execution will build the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "name2index(\"Paul_Erdös\", \"dblp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "name2index(\"Fabien_Mathieu\", \"dblp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index2name(711561, \"dblp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "index2name(149114, \"dblp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Remark:** The ``name2index`` function is very rough. It just tries to match ``name`` as a substring of ``text`` and finds the corresponding index in the delimiter array. It is quite slow and may fail if the name of a researcher is a substring of the name of another researcher, but it will be enough for this practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## List comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You may have already seen *list comprehension* before: it is for example used in some of the function above to convert a text line into a list of neighbors when you parse the adjacency list: ``[int(s) for s in line.split()]``.\n",
    "\n",
    "They are a powerful tool to construct a list by describing how it is built, and you will have to use them in this practical, so you should study the following examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A first simple example: the list of the squares of the integers from 0 to 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[i**2 for i in range(6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One of the interest of list comprehension is that they can be nested. For example, the list of the squares of the 6 first positive odd integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[i**2 for i in [2*k+1 for k in range(6)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A last example of list comprehension, which will be very helpful for the clustering coefficient. Can you figure out what it does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[k for nj in [range(j) for j in range(6)] for k in nj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Power laws in the wild: undirected graphs (DBLP and IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBLP (*DataBase systems and Logic Programming* or *Digital Bibliography & Library Project*) is THE database that records computer science publications. It records authors, conferences, journals... The co-authorship graph of DBLP is a good entry point to study undirected small-worlds.\n",
    "\n",
    "There are multiple versions of the DBLP graph available. As in the previous practical, we will focus on the one available on [KONECT](http://konect.uni-koblenz.de/networks/dblp_coauthor).\n",
    "\n",
    "Let us first see how to compute the size of the Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Write two functions ``build_degree(prefix)`` and ``get_degree(prefix, rebuild = False)`` that return the degrees of a dataset, that is, the number of neighbors of each node. These functions can be built on the same model as the functions ``build_adjacency`` and ``get_adjacency`` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Write two functions ``plot_pmf(prefix, rebuild = False)`` and ``plot_ccdf(prefix, rebuild = False)`` that display the degree distribution of a dataset in a loglog scale. For example, they may build an array ``distribution`` such that the number of nodes that have degree $i$ is ``distribution[i]``. Comment the results in view of the previous parts of this practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (Optional)\n",
    "\n",
    "Redo the previous questions with IMDB dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Power laws in the wild: directed graphs (Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now play with French and English crawls of Wikipedia made in 2013 and available on [WebGraph](http://webgraph.di.unimi.it/). These graphs have been *cleaned*: only links from one article to another article are kept.\n",
    "\n",
    "The two main differences with the (non-oriented) DBLP/IMDB databases are:\n",
    "- The graphs are now *oriented*: a link from $i$ to $j$ does not mean there is a link from $j$ to $i$.\n",
    "- The graphs are bigger. If you didn't optimize your code for DBLP/IMDB, you will probably have to optimize it now. \n",
    "\n",
    "The French crawl is made of three files:\n",
    "- **frwiki-2013.ids** contains the article titles (one per line, $ n $ lines in total). By convention, the index $i$ of an article is its line number in this file (from $ 0 $ to $ n-1 $).\n",
    "- **frwiki-2013.adja** contains the adjacency list of the graph: for each $i = 0, \\ldots, n-1$, line $i$ contains the indices of the articles that are linked by $i$, written in plain ASCII. \n",
    "- **frwiki-2013-t.adja** contains the adjacency list of the transposed graph: for each $i = 0, \\ldots, n-1$, line $i$ contains the indices of the articles that have a link to $i$, written in plain ASCII.\n",
    "\n",
    "The English crawl is provided in a similar way, with the prefix **enwiki-2013** instead of **frwiki-2013**. Note that it is roughly thrice bigger than the French crawl. Feel free to use the dataset(s) you want.\n",
    "\n",
    "The questions are essentially the same as for the DBLP/IMDB datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Give the number of nodes and edges of the dataset(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Write two functions ``build_degrees(prefix)`` and ``get_degrees(prefix, rebuild = False)`` that return two arrays ``indegree`` and ``outdegree`` containing the in and out-degrees in a dataset, respectively. These functions can be built on the same model as the functions ``build_degree`` and ``get_degree`` defined in Question 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Write two functions ``plot_pmfs(prefix, rebuild = False)`` and ``plot_ccdfs(prefix, rebuild = False)`` that display the in and outdegree distributions of a dataset in a loglog scale. Comment the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
